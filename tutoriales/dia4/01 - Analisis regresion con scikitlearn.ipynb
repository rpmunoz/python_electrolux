{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión con Scikit-learn\n",
    "\n",
    "**Autor:** Roberto Muñoz <br />\n",
    "**E-mail:** <rmunoz@metricarts.com> <br />\n",
    "**Github:** <https://github.com/rpmunoz> <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regresión\n",
    "\n",
    "En regresión, la etiqueta es continua, es decir una salida real. Por ejemplo, en astronomía, la tarea de determinar si un objeto es una estrella, una galaxia o un cuásar es un problema de clasificación: la etiqueta viene de tres categorías distintas. Por otro lado, podríamos querer estimar la edad de un objeto basándonos en su imagen: esto sería regresión, porque la etiqueta (edad) es una cantidad continua.\n",
    "\n",
    "Usaremos el dataet diabetes de scikit-learn, el cual consiste de 10 variables fisiológicas (age, sex, weight, blood pressure) medidas en 442 pacientes, y el avance de la enfermedad después de un año.\n",
    "\n",
    "- age: Edad\n",
    "- sex: Sexo\n",
    "- body mass index: Índice de masa corporal\n",
    "- average blood pressure: Presión sanguinea promedio\n",
    "- s1-s6: Mediciones del suero sanguíneo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "diabetes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetes['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un dataframe a partir del dataset cargado desde scikit-learn\n",
    "\n",
    "diabetes_df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "diabetes_df[\"avance\"]=diabetes.target\n",
    "\n",
    "print(\"Tamaño del dataset: \", len(diabetes_df))\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "plot=scatter_matrix(diabetes_df, alpha=0.2, figsize=[15, 15], marker=\".\", diagonal='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(diabetes_df, test_size=0.3)\n",
    "\n",
    "print(\"Tamaño del train set: \", len(df_train))\n",
    "print(\"Tamaño del test set: \", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regresión lineal\n",
    "\n",
    "$$ \\text{min}_{\\mathbf{w}, b} \\sum_i || \\mathbf{w}^\\mathsf{T}\\mathbf{x}_i + b  - y_i||^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Caso simple (1 feature)\n",
    "\n",
    "Partiremos consturyendo un modelo lineal usando el campo bmi, el cual corresponde a la columna numero 2\n",
    "\n",
    "Usaremos la función LinearRegression que permite crear un modelo lineal usando la regresión de mínimos cuadrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array(df_train.iloc[:,2])[np.newaxis].T\n",
    "y_train=np.array(df_train.iloc[:,10])\n",
    "\n",
    "X_test=np.array(df_test.iloc[:,2])[np.newaxis].T\n",
    "y_test=np.array(df_test.iloc[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "print(\"R^2 en entrenamiento: %f\" % regr.score(X_train, y_train))\n",
    "print(\"R^2 en test: %f\" % regr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usaremos el campo bmi para graficar el ajuste lineal\n",
    "\n",
    "plt.scatter(X_test, y_test,  color='black')\n",
    "plt.plot(X_test, y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xlabel('Indice de masa corporal')\n",
    "plt.ylabel('Avance enfermedad')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Caso multivariado (Todos los features)\n",
    "\n",
    "Usaremos todos los features para generar un modelo lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array(df_train.iloc[:,0:10])\n",
    "y_train=np.array(df_train.iloc[:,10])\n",
    "\n",
    "X_test=np.array(df_test.iloc[:,0:10])\n",
    "y_test=np.array(df_test.iloc[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "print(\"R^2 en entrenamiento: %f\" % regr.score(X_train, y_train))\n",
    "print(\"R^2 en test: %f\" % regr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Construcción de curva de aprendizaje\n",
    "\n",
    "Scikit-learn permite calcular la curva de aprendizaje usando la función learning_curve. Es una herramienta que permite determinar cuanto ganamos al agregar mas datos al set de entrenamiento y determinar si el estimador sufre de un error de varianza o un erro de bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(est, X, y):\n",
    "    training_set_size, train_scores, test_scores = learning_curve(est, X, y, train_sizes=np.linspace(.1, 1, 20))\n",
    "    estimator_name = est.__class__.__name__\n",
    "    line = plt.plot(training_set_size, train_scores.mean(axis=1), '--', label=\"puntuaciones de entrenamiento \" + estimator_name)\n",
    "    plt.plot(training_set_size, test_scores.mean(axis=1), '-', label=\"puntuaciones de test \" + estimator_name, c=line[0].get_color())\n",
    "    plt.xlabel('Tamaño del conjunto de entrenamiento')\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylim(-0.1, 1.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(diabetes_df.iloc[:,0:10])\n",
    "y=np.array(diabetes_df.iloc[:,10])\n",
    "\n",
    "plt.figure()\n",
    "plot_learning_curve(LinearRegression(), X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regresión de cresta (*Ridge Regression*, penalización L2)\n",
    "\n",
    "**El estimador de cresta (``Ridge``)** es una regularización simple (llamada regularización L2) para el modelo LinearRegression. En particular, tiene el beneficio de no ser más costoso computacionalmente que la estimación basada en mínimos cuadrados.\n",
    "\n",
    "$$ \\text{min}_{\\mathbf{w},b}  \\sum_i || \\mathbf{w}^\\mathsf{T}\\mathbf{x}_i + b  - y_i||^2  + \\alpha ||\\mathbf{w}||_2^2$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_models = {}\n",
    "training_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for alpha in [100, 10, 1, .01]:\n",
    "    ridge = Ridge(alpha=alpha).fit(X_train, y_train)\n",
    "    training_scores.append(ridge.score(X_train, y_train))\n",
    "    test_scores.append(ridge.score(X_test, y_test))\n",
    "    ridge_models[alpha] = ridge\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(training_scores, label=\"puntuaciones de entrenamiento\")\n",
    "plt.plot(test_scores, label=\"puntuaciones de test\")\n",
    "plt.xticks(range(4), [100, 10, 1, .01])\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cantidad de regularización se ajusta a través del parámetro `alpha` del modelo Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_learning_curve(LinearRegression(), X, y)\n",
    "plot_learning_curve(Ridge(alpha=10), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, -1, 6)\n",
    "print(\"Alphas: \", alphas)\n",
    "\n",
    "ridge = Ridge(alpha=.1)\n",
    "scores = [ridge.set_params(alpha=alpha).fit(X_train, y_train,).score(X_test, y_test) for alpha in alphas]\n",
    "print(\"Score: \", scores) \n",
    "\n",
    "best_alpha = alphas[scores.index(max(scores))]\n",
    "print(\"Best alpha: \", best_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Lasso (penalización L1)\n",
    "**El estimador ``Lasso``** es útil para conseguir imponer dispersión en los coeficientes. En otras palabras, se debería preferir esta penalización si creemos que muchas de las características no son relevantes. Se consigue a través de la regularización L1.\n",
    "\n",
    "$$ \\text{min}_{\\mathbf{w}, b} \\sum_i \\frac{1}{2} || \\mathbf{w}^\\mathsf{T}\\mathbf{x}_i + b  - y_i||^2  + \\alpha ||\\mathbf{w}||_1$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del primer gráfico podemos ver que el campo sex no aporta mucha información en la predicción de la variable avance de enfermedad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso()\n",
    "scores = [lasso.set_params(alpha=alpha).fit(X_train, y_train).score(X_test, y_test) for alpha in alphas]\n",
    "best_alpha = alphas[scores.index(max(scores))]\n",
    "\n",
    "print(\"Best alpha: \", best_alpha)\n",
    "\n",
    "lasso.alpha = best_alpha\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

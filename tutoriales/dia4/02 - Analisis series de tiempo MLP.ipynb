{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis series de tiempo usando MLP\n",
    "\n",
    "**Autor:** Roberto Muñoz <br />\n",
    "**E-mail:** <rmunoz@metricarts.com> <br />\n",
    "**Github:** <https://github.com/rpmunoz> <br />\n",
    "\n",
    "Basado en el análisis de Jason Brownlee https://machinelearningmastery.com/exploratory-configuration-multilayer-perceptron-network-time-series-forecasting/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos un dataset de ventas mensuales de champú entre 1901 y 1903. El dataset contiene 36 observaciones y puede ser descargado desde https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalamos keras y tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos las librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "def parser(x):\n",
    "    return pd.datetime.strptime('190'+x, '%Y-%m')\n",
    "\n",
    "series = pd.read_csv('data/shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split\n",
    "\n",
    "Dividiremos el conjunto de datos de Shampoo Sales en dos partes: uno de entrenamiento y otro de evaluación.\n",
    "\n",
    "Se tomarán los primeros dos años de datos para el conjunto de datos de entrenamiento y el año restante de datos se usará para el conjunto de pruebas.\n",
    "\n",
    "Los modelos se desarrollarán utilizando el conjunto de datos de entrenamiento y harán predicciones sobre el conjunto de datos de prueba.\n",
    "\n",
    "El pronóstico de persistencia (pronóstico ingenuo) en el conjunto de datos de prueba logra un error de 136.761 ventas mensuales de champú. Esto proporciona un límite de rendimiento aceptable más bajo en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = series[0:-12], series[-12:]\n",
    "\n",
    "print(\"\\nTrain\")\n",
    "print(train)\n",
    "\n",
    "print(\"\\nTest\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del modelo\n",
    "\n",
    "Se utilizará un escenario de pronóstico continuo, también denominado validación del modelo de avance.\n",
    "\n",
    "Cada paso del conjunto de datos de prueba se caminará uno a la vez. Se utilizará un modelo para hacer un pronóstico para el paso de tiempo, luego se tomará el valor esperado real del conjunto de prueba y se pondrá a disposición del modelo para el pronóstico en el próximo paso de tiempo.\n",
    "\n",
    "Esto imita un escenario del mundo real donde las nuevas observaciones de Shampoo Sales estarían disponibles cada mes y se usarían en el pronóstico del mes siguiente.\n",
    "\n",
    "Esto será simulado por la estructura del entrenamiento y los conjuntos de datos de prueba.\n",
    "\n",
    "Se recopilarán todos los pronósticos en el conjunto de datos de prueba y se calculará una puntuación de error para resumir la habilidad del modelo. Se utilizará el error cuadrático medio (RMSE), ya que castiga los errores grandes y da como resultado una puntuación que se encuentra en las mismas unidades que los datos de pronóstico, es decir, las ventas mensuales de champú."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los datos\n",
    "\n",
    "Antes de que podamos ajustar un modelo MLP al conjunto de datos, debemos transformar los datos.\n",
    "\n",
    "Las siguientes tres transformaciones de datos se realizan en el conjunto de datos antes de ajustar un modelo y hacer un pronóstico.\n",
    "\n",
    "1. Transforme los datos de la serie temporal para que sean estacionarios. Específicamente, una diferencia de retraso = 1 para eliminar la tendencia creciente en los datos.\n",
    "\n",
    "2. Transforme la serie temporal en un problema de aprendizaje supervisado. Específicamente, la organización de datos en patrones de entrada y salida donde la observación en el paso de tiempo anterior se usa como entrada para pronosticar la observación en el paso de tiempo actual\n",
    "\n",
    "3. Transforme las observaciones para tener una escala específica. Específicamente, para reescalar los datos a valores entre -1 y 1.\n",
    "\n",
    "Estas transformaciones se invierten en pronósticos para devolverlos a su escala original antes del cálculo y la puntuación de error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "    return pd.datetime.strptime('190'+x, '%Y-%m')\n",
    "\n",
    "# frame a sequence as a supervised learning problem\n",
    "def timeseries_to_supervised(data, lag=1):\n",
    "    df = pd.DataFrame(data)\n",
    "    columns = [df.shift(i) for i in range(1, lag+1)]\n",
    "    columns.append(df)\n",
    "    df = pd.concat(columns, axis=1)\n",
    "    return df\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return pd.Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "    return yhat + history[-interval]\n",
    "\n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "    # fit scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(train)\n",
    "    # transform train\n",
    "    train = train.reshape(train.shape[0], train.shape[1])\n",
    "    train_scaled = scaler.transform(train)\n",
    "    # transform test\n",
    "    test = test.reshape(test.shape[0], test.shape[1])\n",
    "    test_scaled = scaler.transform(test)\n",
    "    return scaler, train_scaled, test_scaled\n",
    "\n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, yhat):\n",
    "    new_row = [x for x in X] + [yhat]\n",
    "    array = np.array(new_row)\n",
    "    array = array.reshape(1, len(array))\n",
    "    inverted = scaler.inverse_transform(array)\n",
    "    return inverted[0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo MLP\n",
    "\n",
    "Utilizaremos un modelo MLP base con una capa oculta, una función de activación lineal rectificada en neuronas ocultas y una función de activación lineal en neuronas de salida.\n",
    "\n",
    "Cuando sea posible, se usa un tamaño de lote de 4, con los datos de entrenamiento truncados para garantizar que el número de patrones sea divisible entre 4. En algunos casos se usa un tamaño de lote de 2.\n",
    "\n",
    "Normalmente, el conjunto de datos de entrenamiento se baraja después de cada lote o cada época, lo que puede ayudar a ajustar el conjunto de datos de entrenamiento en problemas de clasificación y regresión. La combinación aleatoria se desactivó para todos los experimentos, ya que parecía tener un mejor rendimiento. Se necesitan más estudios para confirmar este resultado para el pronóstico de series de tiempo.\n",
    "\n",
    "El modelo se ajustará utilizando el eficiente algoritmo de optimización ADAM y la función de pérdida de error cuadrática media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an MLP network to training data\n",
    "def fit_model(train, batch_size, nb_epoch, neurons):\n",
    "    X, y = train[:, 0:-1], train[:, -1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_dim=X.shape[1]))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    history = model.fit(X, y, epochs=nb_epoch, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos función para correr experimentos\n",
    "\n",
    "Definimos la función experiment() para hacer N experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a repeated experiment\n",
    "def experiment(repeats, series, epochs, lag, neurons):\n",
    "    \n",
    "    # transform data to be stationary\n",
    "    raw_values = series.values\n",
    "    diff_values = difference(raw_values, 1)\n",
    "    # transform data to be supervised learning\n",
    "    supervised = timeseries_to_supervised(diff_values, lag)\n",
    "    supervised_values = supervised.values[lag:,:]\n",
    "    \n",
    "    # split data into train and test-sets\n",
    "    train, test = supervised_values[0:-12], supervised_values[-12:]\n",
    "    \n",
    "    # transform the scale of the data\n",
    "    scaler, train_scaled, test_scaled = scale(train, test)\n",
    "    # run experiment\n",
    "    error_scores = list()\n",
    "    \n",
    "    for r in range(repeats):\n",
    "        # fit the model\n",
    "        batch_size = 4\n",
    "        train_trimmed = train_scaled[2:, :]\n",
    "        \n",
    "        model, history = fit_model(train_trimmed, batch_size, epochs, neurons)\n",
    "                \n",
    "        # forecast test dataset\n",
    "        test_reshaped = test_scaled[:,0:-1]\n",
    "        output = model.predict(test_reshaped, batch_size=batch_size)\n",
    "        predictions = list()\n",
    "        for i in range(len(output)):\n",
    "            yhat = output[i,0]\n",
    "            X = test_scaled[i, 0:-1]\n",
    "            # invert scaling\n",
    "            yhat = invert_scale(scaler, X, yhat)\n",
    "            # invert differencing\n",
    "            yhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "            # store forecast\n",
    "            predictions.append(yhat)\n",
    "        # report performance\n",
    "        rmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\n",
    "        print('%d) Test RMSE: %.3f' % (r+1, rmse))\n",
    "        error_scores.append(rmse)\n",
    "        \n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.title('Evolución de la función de costo')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('época')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    return error_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corremos los experimentos\n",
    "\n",
    "Definimos experimentos con numero de épocas igual a 50 y 100. Para cada valor de época hacemos 5 experimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment\n",
    "repeats = 5\n",
    "results = pd.DataFrame()\n",
    "lag = 1\n",
    "neurons = 1\n",
    "\n",
    "# vary training epochs\n",
    "epochs = [50, 100]\n",
    "for e in epochs:\n",
    "    print(\"\\nFitting model with {} epochs\".format(e))\n",
    "    results[str(e)] = experiment(repeats, series, e, lag, neurons)\n",
    "    \n",
    "# summarize results\n",
    "print(results.describe())\n",
    "# save boxplot\n",
    "results.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluar modelo sobre test\n",
    "\n",
    "Usamos el dataset de evaluación para ver el rendimiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on a dataset, returns RMSE in transformed units\n",
    "def evaluate(model, raw_data, scaled_dataset, scaler, offset, batch_size):\n",
    "    # separate\n",
    "    X, y = scaled_dataset[:,0:-1], scaled_dataset[:,-1]\n",
    "    # forecast dataset\n",
    "    output = model.predict(X, batch_size=batch_size)\n",
    "    # invert data transforms on forecast\n",
    "    predictions = list()\n",
    "    for i in range(len(output)):\n",
    "        yhat = output[i,0]\n",
    "        # invert scaling\n",
    "        yhat = invert_scale(scaler, X[i], yhat)\n",
    "        # invert differencing\n",
    "        yhat = yhat + raw_data[i]\n",
    "        # store forecast\n",
    "        predictions.append(yhat)\n",
    "    # report performance\n",
    "    rmse = sqrt(mean_squared_error(raw_data[1:], predictions))\n",
    "    return rmse\n",
    "\n",
    "# fit an MLP network to training data\n",
    "def fit(train, test, raw, scaler, batch_size, nb_epoch, neurons):\n",
    "    X, y = train[:, 0:-1], train[:, -1]\n",
    "    # prepare model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, activation='relu', input_dim=X.shape[1]))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    # fit model\n",
    "    train_rmse, test_rmse = list(), list()\n",
    "    for i in range(nb_epoch):\n",
    "        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "        # evaluate model on train data\n",
    "        raw_train = raw[-(len(train)+len(test)+1):-len(test)]\n",
    "        train_rmse.append(evaluate(model, raw_train, train, scaler, 0, batch_size))\n",
    "        # evaluate model on test data\n",
    "        raw_test = raw[-(len(test)+1):]\n",
    "        test_rmse.append(evaluate(model, raw_test, test, scaler, 0, batch_size))\n",
    "    history = pd.DataFrame()\n",
    "    history['train'], history['test'] = train_rmse, test_rmse\n",
    "    return history\n",
    "\n",
    "# run diagnostic experiments\n",
    "def run():\n",
    "    # config\n",
    "    repeats = 2\n",
    "    n_batch = 4\n",
    "    n_epochs = 100\n",
    "    n_neurons = 1\n",
    "    n_lag = 1\n",
    "    \n",
    "    # load dataset\n",
    "    series = pd.read_csv('data/shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "    # transform data to be stationary\n",
    "    raw_values = series.values\n",
    "    diff_values = difference(raw_values, 1)\n",
    "    # transform data to be supervised learning\n",
    "    supervised = timeseries_to_supervised(diff_values, n_lag)\n",
    "    supervised_values = supervised.values[n_lag:,:]\n",
    "    # split data into train and test-sets\n",
    "    train, test = supervised_values[0:-12], supervised_values[-12:]\n",
    "    # transform the scale of the data\n",
    "    scaler, train_scaled, test_scaled = scale(train, test)\n",
    "    # fit and evaluate model\n",
    "    train_trimmed = train_scaled[2:, :]\n",
    "    \n",
    "    # run diagnostic tests\n",
    "    for i in range(repeats):\n",
    "        history = fit(train_trimmed, test_scaled, raw_values, scaler, n_batch, n_epochs, n_neurons)\n",
    "        plt.plot(history['train'], color='blue', label=('Train' if i==0 else None) )\n",
    "        plt.plot(history['test'], color='orange', label=('Test' if i==0 else None) )\n",
    "        plt.legend()\n",
    "        plt.title('Evolución de la función de costo')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.xlabel('época')\n",
    "\n",
    "        \n",
    "        print('%d) TrainRMSE=%f, TestRMSE=%f' % (i, history['train'].iloc[-1], history['test'].iloc[-1]))\n",
    "    plt.savefig('figures/diagnostic_epochs.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentando con MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment\n",
    "repeats = 5\n",
    "results = pd.DataFrame()\n",
    "lag = 1\n",
    "epochs=100\n",
    "\n",
    "# vary training epochs\n",
    "\n",
    "neurons = [1, 2, 3, 4, 5]\n",
    "for n in neurons:\n",
    "    print(\"\\nFitting model using MLP with {} neurons\".format(n))\n",
    "    results[str(n)] = experiment(repeats, series, epochs, lag, n)\n",
    "    \n",
    "# summarize results\n",
    "print(results.describe())\n",
    "# save boxplot\n",
    "results.boxplot()\n",
    "plt.savefig('figures/boxplot_neurons.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis avanzando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run diagnostic experiments\n",
    "def run():\n",
    "    # config\n",
    "    repeats = 10\n",
    "    n_batch = 2\n",
    "    n_epochs = 200\n",
    "    n_neurons = 3\n",
    "    n_lag = 3\n",
    "    \n",
    "    # load dataset\n",
    "    series = pd.read_csv('data/shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "    # transform data to be stationary\n",
    "    raw_values = series.values\n",
    "    diff_values = difference(raw_values, 1)\n",
    "    # transform data to be supervised learning\n",
    "    supervised = timeseries_to_supervised(diff_values, n_lag)\n",
    "    supervised_values = supervised.values[n_lag:,:]\n",
    "    \n",
    "    # split data into train and test-sets\n",
    "    train, test = supervised_values[0:-12], supervised_values[-12:]\n",
    "    \n",
    "    # transform the scale of the data\n",
    "    scaler, train_scaled, test_scaled = scale(train, test)\n",
    "    \n",
    "    # fit and evaluate model\n",
    "    train_trimmed = train_scaled[2:, :]\n",
    "    \n",
    "    # run diagnostic tests\n",
    "    for i in range(repeats):\n",
    "        history = fit(train_trimmed, test_scaled, raw_values, scaler, n_batch, n_epochs, n_neurons)\n",
    "        plt.plot(history['train'], color='blue', label=('Train' if i==0 else None) )\n",
    "        plt.plot(history['test'], color='orange', label=('Test' if i==0 else None) )\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.title('Evolución de la función de costo')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.xlabel('época')\n",
    "\n",
    "        print('%d) TrainRMSE=%f, TestRMSE=%f' % (i, history['train'].iloc[-1], history['test'].iloc[-1]))\n",
    "\n",
    "    plt.savefig('figures/diagnostic_neurons_lag.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run diagnostic experiments\n",
    "def run():\n",
    "    # config\n",
    "    repeats = 10\n",
    "    n_batch = 4\n",
    "    n_epochs = 200\n",
    "    n_neurons = 3\n",
    "    n_lag = 3\n",
    "    \n",
    "    # load dataset\n",
    "    series = pd.read_csv('data/shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "    # transform data to be stationary\n",
    "    raw_values = series.values\n",
    "    diff_values = difference(raw_values, 1)\n",
    "    # transform data to be supervised learning\n",
    "    supervised = timeseries_to_supervised(diff_values, n_lag)\n",
    "    supervised_values = supervised.values[n_lag:,:]\n",
    "    \n",
    "    # split data into train and test-sets\n",
    "    train, test = supervised_values[0:-12], supervised_values[-12:]\n",
    "    \n",
    "    # transform the scale of the data\n",
    "    scaler, train_scaled, test_scaled = scale(train, test)\n",
    "    \n",
    "    # fit and evaluate model\n",
    "    train_trimmed = train_scaled[2:, :]\n",
    "    \n",
    "    # run diagnostic tests\n",
    "    for i in range(repeats):\n",
    "        history = fit(train_trimmed, test_scaled, raw_values, scaler, n_batch, n_epochs, n_neurons)\n",
    "        plt.plot(history['train'], color='blue', label=('Train' if i==0 else None) )\n",
    "        plt.plot(history['test'], color='orange', label=('Test' if i==0 else None) )\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.title('Evolución de la función de costo')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.xlabel('época')\n",
    "\n",
    "        print('%d) TrainRMSE=%f, TestRMSE=%f' % (i, history['train'].iloc[-1], history['test'].iloc[-1]))\n",
    "\n",
    "    plt.savefig('figures/diagnostic_neurons_lag_batch4.png')\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
